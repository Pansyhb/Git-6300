{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781bf3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.14-py2.py3-none-any.whl (59 kB)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (1.3.4)\n",
      "Collecting pytz>=2022.5\n",
      "  Downloading pytz-2023.2-py2.py3-none-any.whl (502 kB)\n",
      "Requirement already satisfied: cryptography>=3.3.2 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (3.4.8)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.26 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (2.26.0)\n",
      "Collecting beautifulsoup4>=4.11.1\n",
      "  Downloading beautifulsoup4-4.12.0-py3-none-any.whl (132 kB)\n",
      "Collecting multitasking>=0.0.7\n",
      "  Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Collecting frozendict>=2.3.4\n",
      "  Downloading frozendict-2.3.6-cp39-cp39-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (1.24.2)\n",
      "Collecting lxml>=4.9.1\n",
      "  Downloading lxml-4.9.2-cp39-cp39-win_amd64.whl (3.9 MB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.2.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from cryptography>=3.3.2->yfinance) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.20)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (2.0.4)\n",
      "Installing collected packages: pytz, multitasking, lxml, frozendict, beautifulsoup4, yfinance\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2021.3\n",
      "    Uninstalling pytz-2021.3:\n",
      "      Successfully uninstalled pytz-2021.3\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 4.6.3\n",
      "    Uninstalling lxml-4.6.3:\n",
      "      Successfully uninstalled lxml-4.6.3\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.10.0\n",
      "    Uninstalling beautifulsoup4-4.10.0:\n",
      "      Successfully uninstalled beautifulsoup4-4.10.0\n",
      "Successfully installed beautifulsoup4-4.12.0 frozendict-2.3.6 lxml-4.9.2 multitasking-0.0.11 pytz-2023.2 yfinance-0.2.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7092fc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.2)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97cf68a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pysparkNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
      "Collecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=1b00a69fd4e4564e6bebc565bb59ea4a9f7f18f2e120902f8ba07765f33306b8\n",
      "  Stored in directory: c:\\users\\pansy\\appdata\\local\\pip\\cache\\wheels\\6c\\e3\\9b\\0525ce8a69478916513509d43693511463c6468db0de237c86\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3704b62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark[pandas_on_spark] in c:\\users\\pansy\\anaconda3\\lib\\site-packages (3.3.2)Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting plotly\n",
      "  Downloading plotly-5.13.1-py2.py3-none-any.whl (15.2 MB)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from pyspark[pandas_on_spark]) (0.10.9.5)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from pyspark[pandas_on_spark]) (1.24.2)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from pyspark[pandas_on_spark]) (1.3.4)\n",
      "Collecting pyarrow>=1.0.0\n",
      "  Downloading pyarrow-11.0.0-cp39-cp39-win_amd64.whl (20.6 MB)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]) (2023.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.0.5->pyspark[pandas_on_spark]) (1.16.0)\n",
      "Installing collected packages: tenacity, pyarrow, plotly\n",
      "Successfully installed plotly-5.13.1 pyarrow-11.0.0 tenacity-8.2.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pyspark[pandas_on_spark] plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9705971d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10404/2258557711.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSafeLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from pyspark.sql.functions import sum,max,min,mean,count\n",
    "import datetime as dt\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from os.path import abspath\n",
    "\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "with open('cfg.yml') as f:\n",
    "    config = yaml.load(f, Loader = SafeLoader)\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder \\\n",
    "    .master(config['spark']['spark_master'])\\\n",
    "    .appName('gather')\\\n",
    "    .enableHiveSupport()\\\n",
    "    .config('spark.sql.warehouse.dir', warehouse_location)\\\n",
    "    .config(config['spark']['spark_jars'], config['spark']['spark_jars_path'])\\\n",
    "    .config('spark.cores.max', '2')\\\n",
    "    .config('spark.executor.cores', '2')\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "spark\n",
    "\n",
    "url = config['postgres']['url']\n",
    "properties = {\n",
    "    'user': config['postgres']['user'],\n",
    "    'password' : config['postgres']['user'],\n",
    "    'url': url,\n",
    "    'driver': config['postgres']['driver']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1998854",
   "metadata": {},
   "source": [
    "# Training the Model on a Single Stock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c295390",
   "metadata": {},
   "source": [
    "## Retrieve data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "228f7615",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (Temp/ipykernel_10404/3486380448.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\pansy\\AppData\\Local\\Temp/ipykernel_10404/3486380448.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    sentiment = spark.read.jbdc(url = url, 'sentiment', properties = properties).dropDuplicates()\u001b[0m\n\u001b[1;37m                                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "def return_data(ticker_list, from_date, to_date):\n",
    "    sentiment = spark.read.jbdc(url = url, 'sentiment', properties = properties).dropDuplicates()\n",
    "    finance = spark.read.jbdc(url = url, 'company_data', properties = properties).dropDuplicates()\n",
    "    condition = [finance.date == sentiment.date, finance.ticker == sentiment.ticker]\n",
    "    full_data = finance.join(sentiment, condition).fillna(0)\n",
    "    full_data.createOrReplaceTempView('dataset')\n",
    "    df_list = []\n",
    "    for ticker in ticker_list:\n",
    "        try:\n",
    "            working_data = spark.sql(\"Select * from dataset where ticker == \" + str(ticker) + \" & date between \"+str(from_date) + \" and \"+ str(to_date))\n",
    "        except:\n",
    "            print(\"Failed to Retrieve Data from Database for ticker \" + str(ticker) + \". Please load necessary data and retry query\")\n",
    "        df_list.append(working_data.toPandas().sort_values(by = 'date', ascending = True))\n",
    "    return df_list\n",
    "dfs = return_data(['MSFT', 'GOOG'], \"2018-01-01\", \"2023-02-02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f6c41a",
   "metadata": {},
   "source": [
    "## Scale Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e14583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "target_scaler = MinMaxScaler()\n",
    "def scale(df_list):\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    target_scaler = MinMaxScaler()\n",
    "    for df in df_list:\n",
    "        data = df.drop(['ticker'],axis=1)\n",
    "    # features and target columns\n",
    "        target = target_scaler.fit_transform(data['target'])\n",
    "        X_feat = data.drop(['target'], axis = 1)\n",
    "        for col in X_feat.columns:\n",
    "            X_feat[col] = scaler.fit_transform(X_feat[col])\n",
    "    return X_feat, target_scaler, target\n",
    "X_feat, target_scaler = scale(dfs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69929a5",
   "metadata": {},
   "source": [
    "## Format Training Data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cede73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 10 time-steps and 1 output\n",
    " \n",
    "# Split data into X_train and y_train data sets\n",
    "def lstm_split(data,target,steps):\n",
    "      X = []\n",
    "      y = []\n",
    "      # Creating a data structure with 10 time-steps and 1 output\n",
    "      for i in range(10, steps):\n",
    "          X.append(data[i-10:i])\n",
    "          y.append(target[i:i+1])  \n",
    "      return np.array(X),np.array(y)\n",
    " \n",
    "X1,y1 = lstm_split(X_feat,target,len(X_feat))\n",
    " \n",
    "train_split = 0.9\n",
    "split_idx = int(np.ceil(len(X1)*train_split))\n",
    "date_index = X_feat.index\n",
    " \n",
    "X_train,X_test = X1[:split_idx],X1[split_idx:]\n",
    "y_train,y_test = y1[:split_idx],y1[split_idx:]\n",
    "X_train_date,X_test_date = date_index[:split_idx],date_index[split_idx:]\n",
    " \n",
    "print(X1.shape,X_train.shape,X_test.shape,y_test.shape,y_train.shape)\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74ee092",
   "metadata": {},
   "source": [
    "## Constructing the Primary LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM Framework\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.metrics import Precision\n",
    "from keras.optimizers import Adam\n",
    "#!pip install keras_tuner\n",
    "import keras_tuner\n",
    "\n",
    "#define function to create model, optional hyperparameters included to be selected during training\n",
    "LR = 0.05\n",
    "def build_model(hp):\n",
    "  model = Sequential()\n",
    "  hidden = hp.Choice('n_hidden', [0,1,2,3])\n",
    "  model.add(LSTM(units = hp.Int('neurons_visible', min_value = X_train.shape[2], max_value = 100, step = 20),\n",
    "                activation = hp.Choice('activate1', ['sigmoid', 'relu']),\n",
    "                input_shape = (X_train.shape[1], X_train.shape[2]),\n",
    "                return_sequences = True if hidden >0 else False))\n",
    "  #Configure hidden layers based on random search determined hidden layer number\n",
    "  if hidden > 0:\n",
    "    for num in range(hidden):\n",
    "      model.add(Dropout(hp.Float('dropout' +str(num+1), min_value = 0.1, max_value = 0.9, step = 0.3)))\n",
    "      model.add(LSTM(units = hp.Int('neurons_hidden'+str(num+1), min_value = 20, max_value = 50, step = 10),\n",
    "                     activation = 'relu', return_sequences = True if num != hidden else False))\n",
    "      \n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "  #compile the model\n",
    "  model.compile(loss = 'mean_squared_error', optimizer = Adam(lr = LR), metrics = ['mean_squared_error'])\n",
    "  \n",
    "  return model\n",
    "\n",
    "#set learning rate and early stopping callbacks\n",
    "LR_decay = ReduceLROnPlateau('loss', patience=1, verbose=0, \n",
    "                             factor=0.5, min_lr=1e-8)\n",
    "Early_stop = EarlyStopping(monitor='loss', min_delta=0, \n",
    "                           patience=25, verbose=1, mode='auto',\n",
    "                           baseline=0, restore_best_weights=True)\n",
    "#arrange random search class\n",
    "tune = keras_tuner.RandomSearch(build_model, objective = 'val_loss', max_trials = 50, seed = 1)\n",
    "\n",
    "#complete training \n",
    "tune.search(X_train, y_train, epochs = 200, batch_size = 24, validation_data = (X_test, y_test), callbacks = [LR_decay, Early_stop])\n",
    "\n",
    "LSTM_model = tune.get_best_models()[0]\n",
    "LSTM_model.save('LSTM_model.h5')\n",
    "hyperparameters = tune.get_best_hyperparameters(1)[0]\n",
    "print(hyperparameters.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3b4e0",
   "metadata": {},
   "source": [
    "## Constructing the Prophet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c86314",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSFT_Data = finance_data[finance_data['ticker']=='MSFT']\n",
    "# This function is used to get the train data and test data\n",
    "def data_preparation(df):\n",
    "  data=df[[\"date\",\"adj_close\"]]\n",
    "  data = data.rename(columns = {'date':'ds', 'adj_close':'y'})\n",
    "  return data\n",
    "# Define parameter grid to search over\n",
    "param_grid = {\n",
    "    'seasonality_mode': ['additive', 'multiplicative'],\n",
    "    'changepoint_prior_scale': [0.01, 0.1, 1.0],\n",
    "    'seasonality_prior_scale': [0.01, 0.1, 1.0],\n",
    "}\n",
    "\n",
    "# Initialize minimum error and best parameters\n",
    "min_error = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Loop through all parameter combinations\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print('Testing parameters:', params)\n",
    "\n",
    "    # Initialize Prophet model with specified hyperparameters\n",
    "    model = Prophet(**params)\n",
    "    model.fit(data_preparation(MSFT_Data))\n",
    "\n",
    "    # Perform time series cross-validation\n",
    "    df_cv = cross_validation(model=model, initial='1000 days', horizon='10 days', period='10 days')\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    df_metrics = performance_metrics(df_cv)\n",
    "\n",
    "    # Calculate mean cross-validation error\n",
    "    mean_cv_error = df_metrics['mse'].mean()\n",
    "\n",
    "    # Update minimum error and best parameters if new minimum is found\n",
    "    if mean_cv_error < min_error:\n",
    "        min_error = mean_cv_error\n",
    "        best_params = params\n",
    "\n",
    "# Print best hyperparameters and corresponding error\n",
    "print('Best parameters:', best_params)\n",
    "print('Minimum cross-validation error:', min_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the best parameter to fit the model\n",
    "final_model = Prophet(**best_params)\n",
    "final_model.fit(data_preparation(MSFT_Data))\n",
    "\n",
    "future = final_model.make_future_dataframe(periods=100)\n",
    "validation_predict = final_model.predict(future)\n",
    "print(validation_predict[['ds', 'yhat', 'yhat_lower', 'yhat_upper']])\n",
    "fig = final_model.plot(validation_predict)\n",
    "fig1 = final_model.plot_components(validation_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9924e7",
   "metadata": {},
   "source": [
    "## Format Predictions of Sub-Models to Create Final Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37684228",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = load('LSTM_model.h5')\n",
    "preds_dict = {}\n",
    "\n",
    "preds_dict['lstm_pred'] = lstm.predict(X_train)\n",
    "preds_dict['prophet_pred_microsoft'] = f\n",
    "\n",
    "hybrid_train = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1cf90c",
   "metadata": {},
   "source": [
    "## Constructing the Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2444ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "#General LSTM model based on randomsearch results\n",
    "def multiple_models(x_train, y_train):\n",
    "    hybrid = Sequential()\n",
    "    hybrid.add(LSTM(units = 8, activation = \"relu\", input_shape = (X_train.shape[1], X_train.shape[2]), return_sequences = True))\n",
    "    hybrid.add(Dropout(0.5))\n",
    "    hybrid.add(LSTM(units = 12, activation = 'relu', return_sequences = True))\n",
    "    hybrid.add(Dropout(0.5))\n",
    "    hybrid.add(LSTM(units = 12))\n",
    "    hybrid.add(Dropout(0.5))\n",
    "    hybrid.add(Dense(units =1)\n",
    "    hybrid.compile(loss = 'mean_squared_error', optimizer = Adam(lr = LR), metrics = [\"mean_squared_error\"])\n",
    "    Early_stop = EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                              patience=25, verbose=1, mode='auto',\n",
    "                              baseline=0, restore_best_weights=True)\n",
    "    hybrid.fit(x_train, y_train, epochs = 200, batch_size = 24, callbacks = [Early_stop])\n",
    "    return hybrid\n",
    "#train the hybrid model\n",
    "hybrid1 = multiple_models(hybrid_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff6b8b1",
   "metadata": {},
   "source": [
    "# Model Testing Single Company Hybrid Model vs. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287210ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep google test data\n",
    "google = frames[0]\n",
    "google_target_scaler = MinMaxScaler()\n",
    "google_target = google_target_scaler.fit_transform(google[['target']])\n",
    "google_score = google['score']\n",
    "test_ft = google.drop(['target', 'score', 'ticker'], axis = 1)\n",
    "\n",
    "# Normalise the data\n",
    "\n",
    "for col in test_ft.columns:\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    X_feat[col] = scaler.fit_transform(test_ft[[col]])\n",
    "test_ft['score'] = google_score\n",
    "test_ft = np.array(test_ft)\n",
    "google_ft, google_target = lstm_split(test_ft,google_target,len(google_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.evaluate_model()\n",
    "hybrid1.evaluate_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d157e6",
   "metadata": {},
   "source": [
    "# Expanded Model: Multiple Companies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c55dfd",
   "metadata": {},
   "source": [
    "## Retrieve Data from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = return_data(['MSFT', 'GOOG', 'AMZN', \"NFLX\", \"TSLA\"], \"2018-01-01\", \"2023-02-02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce64bd97",
   "metadata": {},
   "source": [
    "## Create Sub-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f259fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lists(list_of_df):\n",
    "  training_list = []\n",
    "  target_list = []\n",
    "  for i in list_of_df:\n",
    "    data = frames[i]\n",
    "    score = data['score']\n",
    "    target = data['target']\n",
    "    training = data.drop(['target', 'ticker', 'score'], axis = 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    for col in training.columns:\n",
    "      training[col] = scaler.fit_transform(training[col])\n",
    "    training['score'] = score\n",
    "    data_x, data_y = lstm_split(training, target, 10)\n",
    "    training_list.append(data_x)\n",
    "    target_list.append(data_y)\n",
    "  return training_list, target_list\n",
    "\n",
    "#Train each of the lstm models on training companies\n",
    "training_list, target_list = create_lists(frames)\n",
    "for tick, x_train, y_train in zip(ticker_list, training_list, target_list):\n",
    "  model = multiple_models(tick, x_train, y_train)\n",
    "  model.save('/models/'+tick+'_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Train Data for Hybrid Model Using Microsoft Predictions\n",
    "def hybrid_train(direct, ticker_list, hybrid_train):\n",
    "    preds_dict = {}\n",
    "    path = direct\n",
    "    for num, model in enumerate(os.listdir(path)):\n",
    "        model = load_model(model)\n",
    "        prediction = model.predict(hybrid_train)\n",
    "        preds_dict[ticker_list[num]] = prediction\n",
    "    preds_df = pd.DatFrame(preds_dict)\n",
    "    return preds_df\n",
    "\n",
    "ticker_list = [\"MSFT\", \"NFLX\", \"AMZN\", \"TSLA\"]\n",
    "hybrid_training_data =  hybrid_train(\"/models/\", ticker_list, X_train) \n",
    "hybrid_training_data['prophet'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5cda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_expanded = multiple_models(hybrid_training_data, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c166d9",
   "metadata": {},
   "source": [
    "## Test Hybrid Model Using Microsoft Validation Data and Google Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e1f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "google_test = {}\n",
    "microsoft_test = {}\n",
    "path = '/models/'\n",
    "for num, model in enumerate(os.listdir(path)):\n",
    "  model = load_model(model)\n",
    "  goog_preds = model.predict(google_ft)\n",
    "  micro_preds = model.predict(X_test)\n",
    "  google_test[ticker_list[num]] = goog_preds\n",
    "  microsoft_test[ticker_list[num]] = micro_preds\n",
    "google_test = pd.DataFrame(google_test)\n",
    "microsoft_test = pd.DataFrame(microsoft_test)\n",
    "google_test['prophet'] = \n",
    "microsoft_test['prophet'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabe931",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8920ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
