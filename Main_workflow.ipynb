{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9705971d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 17:57:29 WARN Utils: Your hostname, cis6180 resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/03/28 17:57:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/03/28 17:57:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from pyspark.sql.functions import sum,max,min,mean,count\n",
    "import datetime as dt\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from os.path import abspath\n",
    "\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "with open('cfg.yml') as f:\n",
    "    config = yaml.load(f, Loader = SafeLoader)\n",
    "\n",
    "    #create spark connection\n",
    "findspark.init()\n",
    "spark = SparkSession.builder \\\n",
    "    .master(config['spark']['spark_master'])\\\n",
    "    .appName('retrieve')\\\n",
    "    .enableHiveSupport()\\\n",
    "    .config('spark.sql.warehouse.dir', warehouse_location)\\\n",
    "    .config(config['spark']['spark_jars'], config['spark']['spark_jars_path'])\\\n",
    "    .config('spark.cores.max', '2')\\\n",
    "    .config('spark.executor.cores', '2')\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "spark\n",
    "\n",
    "#create database config details\n",
    "url = config['postgres']['url']\n",
    "properties = {\n",
    "    'user': config['postgres']['user'],\n",
    "    'password' : config['postgres']['user'],\n",
    "    'url': url,\n",
    "    'driver': config['postgres']['driver']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8efee241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cis6180/wd_main/Final_project_files'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1998854",
   "metadata": {},
   "source": [
    "# Training the Model on a Single Stock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c295390",
   "metadata": {},
   "source": [
    "## Retrieve data from database"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 3,
   "id": "228f7615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {

     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT (6966, 21)\n",
      "MSFT (6925, 20)\n",
      "MSFT (1802, 20)\n",
      "GOOG (3678, 21)\n",
      "GOOG (3641, 20)\n",

      "GOOG (1802, 20)\n",
      "AMZN (4599, 21)\n",
      "AMZN (4309, 20)\n",
      "AMZN (1802, 20)\n",
      "TSLA (5467, 21)\n",
      "TSLA (5145, 20)\n",
      "TSLA (1802, 20)\n",
      "NFLX (3395, 21)\n",
      "NFLX (3105, 20)\n",
      "NFLX (1802, 20)\n",
      "23/03/27 09:55:55 WARN HeartbeatReceiver: Removing executor 0 with no recent heartbeats: 30290708 ms exceeds timeout 120000 ms\n",
      "23/03/27 09:55:55 ERROR TaskSchedulerImpl: Lost executor 0 on 10.0.2.15: Executor heartbeat timed out after 30290708 ms\n",
      "23/03/27 09:56:01 ERROR TaskSchedulerImpl: Lost executor 1 on 10.0.2.15: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "#retrieve data from database\n",
    "def return_data(ticker_list, from_date, to_date):\n",
    "    sentiment = spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", \"jdbc:postgresql://localhost:5432/financials\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"sentiment\") \\\n",
    "        .option(\"user\", \"adam\").option(\"password\", \"green\").load()\n",
    "    finance = spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", \"jdbc:postgresql://localhost:5432/financials\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"company_data\") \\\n",
    "        .option(\"user\", \"adam\").option(\"password\", \"green\").load()\n",
    "    full_data = finance.join(sentiment, ['date', 'ticker'], 'left').fillna(0)\n",
    "    full_data = full_data.toPandas()\n",
    "    df_list = []\n",
    "    for ticker in ticker_list:\n",
    "        working_data = full_data[full_data['ticker'] == ticker]\n",
    "        print(ticker, working_data.shape)\n",
    "        working_data = working_data.set_index('date').sort_values(by = 'date', ascending = True).loc[from_date:to_date,]\n",
    "        print(ticker,  working_data.shape)\n",
    "        working_data = working_data[~working_data.index.duplicated()]\n",
    "        print(ticker, working_data.shape)\n",
    "        working_data.to_csv('data/'+ticker+'_dataframe.csv')\n",
    "        df_list.append(working_data)\n",
    
    "return_data(['MSFT', 'GOOG'], \"2016-01-01\", \"2023-03-01\")"

    "return_data(['MSFT', 'GOOG', 'AMZN', 'TSLA', 'NFLX'], \"2016-01-01\", \"2023-03-01\")"

   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f6c41a",
   "metadata": {},
   "source": [
    "## Scale Numeric Columns"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 3,
   "id": "6e14583c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cis6180/anaconda3/lib/python3.9/site-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Function to scale columns for data where scaling of predictions is eventually needed\n",
    "def scale(df):\n",

    "    df = df.to_pandas()\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    target_scaler = MinMaxScaler()\n",
    "    data = df.drop(['ticker'],axis=1)\n",
    "    # features and target columns\n",
    "    target = target_scaler.fit_transform(data[['target']]).flatten()\n",
    "    X_feat = data.drop(['target'], axis = 1)\n",
    "    for col in X_feat.columns:\n",
    "        X_feat[col] = scaler.fit_transform(X_feat[[col]])\n",
    "    return X_feat, target_scaler, target\n",
    "X_feat, target_scaler, target = scale(df_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69929a5",
   "metadata": {},
   "source": [
    "## Format Training Data for LSTM"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 4,
   "id": "07cede73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1792, 10, 18) (1613, 10, 18) (179, 10, 18) (179, 1) (1613, 1)\n"

     ]
    }
   ],
   "source": [
    "# Creating a data structure with 10 time-steps and 1 output\n",
    "# Split data into X_train and y_train data sets\n",
    "def lstm_split(data,target,steps):\n",
    "      X = []\n",
    "      y = []\n",
    "      # Creating a data structure with 10 time-steps and 1 output\n",
    "      for i in range(10, steps):\n",
    "          X.append(data[i-10:i])\n",
    "          y.append(target[i:i+1])  \n",
    "      return np.array(X),np.array(y)\n",
    " \n",
    "X1,y1 = lstm_split(X_feat,target,len(X_feat))\n",
    "\n",
    "#Define Training and Test Datasets\n",
    "train_split = 0.9\n",
    "split_idx = int(np.ceil(len(X1)*train_split))\n",
    "date_index = X_feat.index\n",
    " \n",
    "X_train,X_test = X1[:split_idx],X1[split_idx:]\n",
    "y_train,y_test = y1[:split_idx],y1[split_idx:]\n",
    "X_train_date,X_test_date = date_index[:split_idx],date_index[split_idx:]\n",
    " \n",
    "print(X1.shape,X_train.shape,X_test.shape,y_test.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74ee092",
   "metadata": {},
   "source": [
    "## Constructing the Primary LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1291ac7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9005/2709478604.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#LSTM Framework\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python'"

     ]
    }
   ],
   "source": [
    "#LSTM Framework\n",
    "\n",

    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.metrics import Precision\n",
    "from keras.optimizers import Adam\n",
    "#!pip install keras_tuner\n",
    "import keras_tuner\n",
    "\n",
    "#define function to create model, optional hyperparameters included to be selected during training\n",
    "LR = 0.05\n",
    "def build_model(hp):\n",
    "  model = Sequential()\n",
    "  hidden = hp.Choice('n_hidden', [0,1,2,3])\n",
    "  model.add(LSTM(units = hp.Int('neurons_visible', min_value = X_train.shape[2], max_value = 100, step = 20),\n",
    "                activation = hp.Choice('activate1', ['sigmoid', 'relu']),\n",
    "                input_shape = (X_train.shape[1], X_train.shape[2]),\n",
    "                return_sequences = True if hidden >0 else False))\n",
    "  #Configure hidden layers based on random search determined hidden layer number\n",
    "  if hidden > 0:\n",
    "    for num in range(hidden):\n",
    "      model.add(Dropout(hp.Float('dropout' +str(num+1), min_value = 0.1, max_value = 0.9, step = 0.3)))\n",
    "      model.add(LSTM(units = hp.Int('neurons_hidden'+str(num+1), min_value = 20, max_value = 50, step = 10),\n",
    "                     activation = 'relu', return_sequences = True if num != hidden else False))\n",
    "      \n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "  #compile the model\n",
    "  model.compile(loss = 'mean_squared_error', optimizer = Adam(lr = LR), metrics = ['mean_squared_error'])\n",
    "  \n",
    "  return model\n",
    "\n",
    "#set learning rate and early stopping callbacks\n",
    "LR_decay = ReduceLROnPlateau('loss', patience=1, verbose=0, \n",
    "                             factor=0.5, min_lr=1e-8)\n",
    "Early_stop = EarlyStopping(monitor='loss', min_delta=0, \n",
    "                           patience=25, verbose=1, mode='auto',\n",
    "                           baseline=0, restore_best_weights=True)\n",
    "#arrange random search class\n",
    "tune = keras_tuner.RandomSearch(build_model, objective = 'val_loss', max_trials = 50, seed = 1)\n",
    "\n",
    "#complete training \n",
    "tune.search(X_train, y_train, epochs = 200, batch_size = 24, validation_data = (X_test, y_test), callbacks = [LR_decay, Early_stop])\n",
    "\n",
    "LSTM_model = tune.get_best_models()[0]\n",
    "LSTM_model.save('/models/LSTM_model1.h5')\n",
    "hyperparameters = tune.get_best_hyperparameters()[0]\n",
    "print(hyperparameters.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3b4e0",
   "metadata": {},
   "source": [
    "## Constructing the Prophet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c86314",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSFT_Data = finance_data[finance_data['ticker']=='MSFT']\n",
    "# This function is used to get the train data and test data\n",
    "def data_preparation(df):\n",
    "  data=df[[\"date\",\"adj_close\"]]\n",
    "  data = data.rename(columns = {'date':'ds', 'adj_close':'y'})\n",
    "  return data\n",
    "# Define parameter grid to search over\n",
    "param_grid = {\n",
    "    'seasonality_mode': ['additive', 'multiplicative'],\n",
    "    'changepoint_prior_scale': [0.01, 0.1, 1.0],\n",
    "    'seasonality_prior_scale': [0.01, 0.1, 1.0],\n",
    "}\n",
    "\n",
    "# Initialize minimum error and best parameters\n",
    "min_error = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Loop through all parameter combinations\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print('Testing parameters:', params)\n",
    "\n",
    "    # Initialize Prophet model with specified hyperparameters\n",
    "    model = Prophet(**params)\n",
    "    model.fit(data_preparation(MSFT_Data))\n",
    "\n",
    "    # Perform time series cross-validation\n",
    "    df_cv = cross_validation(model=model, initial='1000 days', horizon='10 days', period='10 days')\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    df_metrics = performance_metrics(df_cv)\n",
    "\n",
    "    # Calculate mean cross-validation error\n",
    "    mean_cv_error = df_metrics['mse'].mean()\n",
    "\n",
    "    # Update minimum error and best parameters if new minimum is found\n",
    "    if mean_cv_error < min_error:\n",
    "        min_error = mean_cv_error\n",
    "        best_params = params\n",
    "\n",
    "# Print best hyperparameters and corresponding error\n",
    "print('Best parameters:', best_params)\n",
    "print('Minimum cross-validation error:', min_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the best parameter to fit the model\n",
    "final_model = Prophet(**best_params)\n",
    "final_model.fit(data_preparation(MSFT_Data))\n",
    "\n",
    "future = final_model.make_future_dataframe(periods=100)\n",
    "validation_predict = final_model.predict(future)\n",
    "print(validation_predict[['ds', 'yhat', 'yhat_lower', 'yhat_upper']])\n",
    "fig = final_model.plot(validation_predict)\n",
    "fig1 = final_model.plot_components(validation_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9924e7",
   "metadata": {},
   "source": [
    "## Format Predictions of Sub-Models to Create Final Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37684228",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = load('LSTM_model1.h5')\n",
    "preds_dict = {}\n",
    "\n",
    "preds_dict['lstm_pred'] = lstm.predict(X_train)\n",
    "preds_dict['prophet_pred_microsoft'] = \n",
    "\n",
    "hybrid_train = pd.DataFrame(preds_dict)\n",
    "hybrid_train,  = lstm_split(hybrid_train, y_train, len(hybrid_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1cf90c",
   "metadata": {},
   "source": [
    "## Constructing the Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2444ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "\n",
    "#General LSTM model based on randomsearch results\n",
    "def multiple_models(x_train, y_train):\n",
    "    hybrid = Sequential()\n",
    "    hybrid.add(LSTM(units = 8, activation = \"relu\", input_shape = (X_train.shape[1], X_train.shape[2]), return_sequences = True))\n",
    "    hybrid.add(Dropout(0.5))\n",
    "    hybrid.add(LSTM(units = 12, activation = 'relu', return_sequences = True))\n",
    "    hybrid.add(Dropout(0.5))\n",
    "    hybrid.add(LSTM(units = 12))\n",
    "    hybrid.add(Dropout(0.5))\n",
    "    hybrid.add(Dense(units =1))\n",
    "    hybrid.compile(loss = 'mean_squared_error', optimizer = Adam(lr = LR), metrics = [\"mean_squared_error\"])\n",
    "    Early_stop = EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                              patience=25, verbose=1, mode='auto',\n",
    "                              baseline=0, restore_best_weights=True)\n",
    "    hybrid.fit(x_train, y_train, epochs = 200, batch_size = 24, callbacks = [Early_stop])\n",
    "    return hybrid\n",
    "#train the hybrid model\n",
    "hybrid1 = multiple_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff6b8b1",
   "metadata": {},
   "source": [
    "# Model Testing Hybrid Model vs. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287210ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep google test data\n",
    "google_feat, google_scaler, google_target = scale(dfs[1])\n",
    "\n",
    "# Normalise the data\n",
    "google_ft, google_target = lstm_split(test_ft,google_target,len(google_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.evaluate_model()\n",
    "hybrid1.evaluate_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d157e6",
   "metadata": {},
   "source": [
    "# Expanded Model: Multiple Companies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c55dfd",
   "metadata": {},
   "source": [
    "## Retrieve Data from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve data for five comapnies of interest\n",
    "#google will be used as purely test data\n",
    "frames = return_data(['MSFT','AMZN', \"NFLX\", \"TSLA\"], \"2018-01-01\", \"2023-02-02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce64bd97",
   "metadata": {},
   "source": [
    "## Create Sub-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f259fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists to hold configured training data for each of the four comapnies of interest\n",
    "def create_data_lists(list_of_df):\n",
    "  training_list = []\n",
    "  target_list = []\n",
    "  for i in list_of_df:\n",
    "    data = list_of_df[i]\n",
    "    target = data['target']\n",
    "    training = data.drop(['target', 'ticker'], axis = 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    for col in training.columns:\n",
    "      training[col] = scaler.fit_transform(training[col])\n",
    "    data_x, data_y = lstm_split(training, target, len(training))\n",
    "    training_list.append(data_x)\n",
    "    target_list.append(data_y)\n",
    "  return training_list, target_list\n",
    "\n",
    "#Train each of the lstm models on training companies\n",
    "training_list, target_list = create_data_lists(frames)\n",
    "for tick, x_train, y_train in zip(ticker_list, training_list, target_list):\n",
    "  model = multiple_models(tick, x_train, y_train)\n",
    "  model.save('/models/'+tick+'_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Train Data for Hybrid Model Using Microsoft Predictions\n",
    "def hybrid_train(direct, ticker_list, hybrid_train):\n",
    "    preds_dict = {}\n",
    "    path = direct\n",
    "    for num, model in enumerate(os.listdir(path)):\n",
    "        model = load_model(model)\n",
    "        prediction = model.predict(hybrid_train)\n",
    "        preds_dict[ticker_list[num]] = prediction\n",
    "    preds_df = pd.DatFrame(preds_dict)\n",
    "    return preds_df\n",
    "\n",
    "ticker_list = [\"MSFT\", \"NFLX\", \"AMZN\", \"TSLA\"]\n",
    "hybrid_training_data =  hybrid_train(\"/models/\", ticker_list, X_train) \n",
    "hybrid_training_data['prophet'] = \n",
    "hybrid_training_data = lstm_split(hybrid_training_data, y_train, len(hybrid_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5cda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and train the expanded hybrid model\n",
    "hybrid_expanded = multiple_models(hybrid_training_data, y_train)\n",
    "hybrid_expanded.save('/models/hybrid_expanded.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c166d9",
   "metadata": {},
   "source": [
    "## Test Hybrid Model Using Microsoft Validation Data and Google Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e1f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure hybrid model testing data based on predictions from each of the four underlying models and prophet\n",
    "from os import listdir\n",
    "google_test = {}\n",
    "microsoft_test = {}\n",
    "path = '/models/'\n",
    "for num, model in enumerate(os.listdir(path)):\n",
    "  model = load_model(model)\n",
    "  goog_preds = model.predict(google_ft)\n",
    "  micro_preds = model.predict(X_test)\n",
    "  google_test[ticker_list[num]] = goog_preds\n",
    "  microsoft_test[ticker_list[num]] = micro_preds\n",
    "google_test = pd.DataFrame(google_test)\n",
    "microsoft_test = pd.DataFrame(microsoft_test)\n",
    "google_test['prophet'] = \n",
    "microsoft_test['prophet'] = \n",
    "google_test = lstm_split(google_test, google_target, len(google_test))\n",
    "microsoft_test = lstm_split(microsoft_test, y_test, len(microsoft_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737db613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict google and microsoft test data using the trained hybrid model\n",
    "hybrid_expanded = load_model('/models/hybrid_expanded.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabe931",
   "metadata": {},
   "source": [
    "# Compare Single Company vs. Multi-Company Hybrid Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8920ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4d8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
