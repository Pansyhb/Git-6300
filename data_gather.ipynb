{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dded0768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\users\\pansy\\anaconda3\\lib\\site-packages (0.2.14)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (2.3.6)\n",
      "Requirement already satisfied: cryptography>=3.3.2 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (3.4.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (4.12.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (2023.2)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.26 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (2.26.0)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (4.9.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (1.24.2)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (1.5.3)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.2.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from cryptography>=3.3.2->yfinance) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.20)\n",
      "Requirement already satisfied: webencodings in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (2.0.4)\n",
      "Requirement already satisfied: pyspark in c:\\users\\pansy\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.5)\n",
      "Requirement already satisfied: findspark in c:\\users\\pansy\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: dateparser in c:\\users\\pansy\\anaconda3\\lib\\site-packages (1.1.8)\n",
      "Requirement already satisfied: tzlocal in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from dateparser) (4.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from dateparser) (2023.2)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from dateparser) (2021.8.3)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from dateparser) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from python-dateutil->dateparser) (1.16.0)\n",
      "Requirement already satisfied: pytz-deprecation-shim in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from tzlocal->dateparser) (0.1.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from tzlocal->dateparser) (2023.2)\n",
      "Requirement already satisfied: vaderSentiment in c:\\users\\pansy\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pansy\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pansy\\anaconda3\\lib\\site-packages\\pyspark\\pandas\\__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pansy\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8520\\28762463.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'spark'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'spark_master'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gather'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pansy\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pansy\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pansy\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    193\u001b[0m             )\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             self._do_init(\n",
      "\u001b[1;32mc:\\Users\\pansy\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pansy\\anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "!pip install yfinance\n",
    "!pip install pyspark\n",
    "!pip install findspark\n",
    "!pip install dateparser\n",
    "!pip install vaderSentiment\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from pyspark.sql.functions import sum,max,min,mean,count\n",
    "import datetime as dt\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as ps\n",
    "import findspark\n",
    "\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from os.path import abspath\n",
    "\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "with open('cfg.yml') as f:\n",
    "    config = yaml.load(f, Loader = SafeLoader)\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder \\\n",
    "    .master(config['spark']['spark_master'])\\\n",
    "    .appName('gather')\\\n",
    "    .enableHiveSupport()\\\n",
    "    .config('spark.sql.warehouse.dir', warehouse_location)\\\n",
    "    .config(config['spark']['spark_jars'], config['spark']['spark_jars_path'])\\\n",
    "    .config('spark.cores.max', '2')\\\n",
    "    .config('spark.executor.cores', '2')\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "spark \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = config['postgres']['url']\n",
    "props = {\n",
    "    'user': config['postgres']['user'],\n",
    "    'password' : config['postgres']['user'],\n",
    "    'url': url,\n",
    "    'driver': config['postgres']['driver']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca7963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve headlines from financial post\n",
    "headers = {'User-Agent':\n",
    "\t'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:108.0) Gecko/20100101 Firefox/108.0'}\n",
    "def gather_headlines(company_name, ticker):\n",
    "    headlines = []\n",
    "    dates = []\n",
    "    for i in range(10, 30000, 10):    # Running for-loop\n",
    "        info_url = \"https://financialpost.com/search/?search_text=\"+company_name +\"&date_range=-3650d&sort=asc&from=\"+str(i)\n",
    "        page = requests.get(info_url, headers = headers)\n",
    "        parser = bs(page.content, \"html.parser\" )\n",
    "        date = parser.body.find_all('div', attrs={'class': 'article-card__meta-bottom'})\n",
    "        for span in date:\n",
    "            dates.append(span.text.split(\"   \")[1])\n",
    "        headline = parser.body.find_all('h3', class_ = 'article-card__headline text-size--extra-large--sm-up')\n",
    "        for x in headline:\n",
    "            headlines.append(x.text)\n",
    "    dates = dates[:len(headlines)]\n",
    "    file = {'date' : dates, \"headline\" : headlines}\n",
    "    file = pd.DataFrame(file)\n",
    "    print(file.head())\n",
    "    file['ticker'] = ticker\n",
    "    return file\n",
    "\n",
    "#calculate sentiment scores for each headlines and append to dataset\n",
    "def analyze_sent(df):\n",
    "    analyze_obj = SentimentIntensityAnalyzer()\n",
    "    df['sentiment']=df['headline'].apply(lambda headline: analyze_obj.polarity_scores(str(headline))['compound'])\n",
    "    df.fillna(0, inplace = True)\n",
    "    return df\n",
    "\n",
    "def final_sentiment(df):\n",
    "    return df.withColumn(\"sent_score\", df.mean_sentiment*(df.headline_count**2)).drop('headline', 'headline_count', 'mean_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d5c77b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date                                           headline\n",
      "0   April 9, 2013   The Double Dragon II remake is so bad you sho...\n",
      "1   April 9, 2013   Microsoft, Nokia file EU antitrust complaint ...\n",
      "2  April 10, 2013   Canadian students show off their games at Lev...\n",
      "3  April 10, 2013   Personal computer shipments shrink 14% in wor...\n",
      "4  April 11, 2013         What you need to know before markets open \n",
      "             date                                           headline\n",
      "0  April 24, 2013   Kindle TV? Amazon said to plan set-top box fo...\n",
      "1  April 25, 2013         CIPO's Amazon guidelines raise new issues \n",
      "2  April 25, 2013   Amazon beats estimates as investments in digi...\n",
      "3  April 25, 2013   Shoppers Drug Mart ups pharmacy market share ...\n",
      "4  April 26, 2013   Amazon shares fall in biggest decline since F...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date                                                                                   headline ticker\n",
      "0  2013-04-09   The Double Dragon II remake is so bad you shouldn't even waste time reading this review    MSFT\n",
      "1  2013-04-09                          Microsoft, Nokia file EU antitrust complaint over Google Android    MSFT\n",
      "2  2013-04-10                                   Canadian students show off their games at Level Up 2013    MSFT\n",
      "3  2013-04-10                              Personal computer shipments shrink 14% in worst-ever decline    MSFT\n",
      "4  2013-04-11                                                 What you need to know before markets open    MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cis6180/anaconda3/lib/python3.9/site-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+---------+\n",
      "|      date|            headline|ticker|sentiment|\n",
      "+----------+--------------------+------+---------+\n",
      "|2013-04-09| The Double Drago...|  MSFT|   -0.487|\n",
      "|2013-04-09| Microsoft, Nokia...|  MSFT|   -0.296|\n",
      "|2013-04-10| Canadian student...|  MSFT|      0.0|\n",
      "|2013-04-10| Personal compute...|  MSFT|      0.0|\n",
      "|2013-04-11| What you need to...|  MSFT|      0.0|\n",
      "|2013-04-11| Microsoft's Wind...|  MSFT|  -0.7579|\n",
      "|2013-04-11| Microsoft falls ...|  MSFT|  -0.4767|\n",
      "|2013-04-11| Electronic Arts ...|  MSFT|      0.0|\n",
      "|2013-04-12| 4.12.13: BlackBe...|  MSFT|      0.0|\n",
      "|2013-04-12| Motocross Madnes...|  MSFT|  -0.4939|\n",
      "|2013-04-12| Who says account...|  MSFT|  -0.3182|\n",
      "|2013-04-15| 4.15.13: Gold an...|  MSFT|      0.0|\n",
      "|2013-04-15| Microsoft smartw...|  MSFT|      0.0|\n",
      "|2013-04-16| Facebook, Apple ...|  MSFT|      0.0|\n",
      "|2013-04-16| Facebook Home se...|  MSFT|      0.0|\n",
      "|2013-04-17| Buying defensive...|  MSFT|  -0.1531|\n",
      "|2013-04-17| 4.17.13: Stickin...|  MSFT|      0.0|\n",
      "|2013-04-17| TSX tumbles as g...|  MSFT|  -0.2023|\n",
      "|2013-04-17| The bull case fo...|  MSFT|   0.5216|\n",
      "|2013-04-17| Learning to use ...|  MSFT|      0.0|\n",
      "+----------+--------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/26 02:45:24 WARN HeartbeatReceiver: Removing executor 0 with no recent heartbeats: 28705741 ms exceeds timeout 120000 ms\n",
      "23/03/26 02:45:25 ERROR TaskSchedulerImpl: Lost executor 0 on 10.0.2.15: worker lost\n"
     ]
    }
   ],
   "source": [
    "import dateparser\n",
    "ticker_list = ['MSFT', 'AMZN']\n",
    "company_list = ['microsoft', 'amazon']\n",
    "\n",
    "\n",
    "def process_headlines(ticker_list, company_list):\n",
    "    dfs = []\n",
    "    for tick, company in zip(ticker_list, company_list):\n",
    "        data = gather_headlines(company, tick)\n",
    "        dfs.append(data)\n",
    "    full_df = pd.concat(dfs)\n",
    "    dates = []\n",
    "    for index, row in full_df.iterrows():\n",
    "        date = dateparser.parse(row['date'], date_formats = [\"%d-%m-%y\"])\n",
    "        dates.append(date.date())\n",
    "    full_df['date'] = dates\n",
    "    full_df = ps.from_pandas(full_df)\n",
    "    print(full_df.head())\n",
    "    full_df = analyze_sent(full_df)\n",
    "    full_df = full_df.to_spark()\n",
    "    full_df.show()\n",
    "    aggregated = full_df.groupBy('date', 'ticker').agg(count('headline').alias('headline_count'), mean('sentiment').alias(\"mean_sentiment\"))\n",
    "    final_news = final_sentiment(aggregated) \n",
    "    final_news.write.format(\"jdbc\")\\\n",
    "        .option(\"url\", \"jdbc:postgresql://localhost:5432/financials\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"sentiment\") \\\n",
    "        .option(\"user\", \"adam\").option(\"password\", \"green\").mode('append').save()\n",
    "\n",
    "process_headlines(ticker_list, company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efe59c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_financials(ticker, start):\n",
    "    time_delt = dt.timedelta(days = 150)\n",
    "    start_day = start - time_delt\n",
    "    data = yf.download(str(ticker), start_day)\n",
    "    data['ticker'] = ticker\n",
    "    data = data.reset_index()\n",
    "    data = data.rename(columns = {'Date':'date', 'Open':'open', 'High':'high', 'Low':'low', 'Close':'close', 'Adj Close': 'adj_close', 'Volume':'volume'})\n",
    "    print('success!')\n",
    "    return data\n",
    "                       \n",
    "                       \n",
    "def EWMA(data, ndays): \n",
    "    EMA = pd.Series(data['close'].ewm(span = ndays, min_periods = ndays - 1).mean(), \n",
    "                 name = 'EWMA_' + str(ndays)) \n",
    "    data = data.join(EMA) \n",
    "    return data\n",
    "\n",
    "def rsi(close, periods = 14):\n",
    "    \n",
    "    close_delta = close.diff()\n",
    "\n",
    "    # Make two series: one for lower closes and one for higher closes\n",
    "    up = close_delta.clip(lower=0)\n",
    "    down = -1 * close_delta.clip(upper=0)\n",
    "    \n",
    "    ma_up = up.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "    ma_down = down.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "\n",
    "    rsi = ma_up / ma_down\n",
    "    rsi = 100 - (100/(1 + rsi))\n",
    "    return rsi\n",
    "\n",
    "def BBANDS(data, window):\n",
    "    MA = data.close.rolling(window).mean()\n",
    "    SD = data.close.rolling(window).std()\n",
    "    data['MiddleBand'] = MA\n",
    "    data['UpperBand'] = MA + (2 * SD) \n",
    "    data['LowerBand'] = MA - (2 * SD)\n",
    "    return data\n",
    "\n",
    "def prep_financials(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    #df.set_index('date')\n",
    "    df['target'] = (df['close'].shift(-1))\n",
    "    df['tenmda'] = df['close'].rolling(10).mean()\n",
    "    df['twentymda'] = df['close'].rolling(20).mean()\n",
    "    df['fiftymda'] = df['close'].rolling(50).mean()\n",
    "    df['hundredmda'] = df['close'].rolling(100).mean()\n",
    "    df = EWMA(df, 20)\n",
    "    df = EWMA(df, 50) \n",
    "    df = EWMA(df, 100)\n",
    "    df['rsi'] = rsi(df['close'])\n",
    "    df = BBANDS(df, 40)\n",
    "    df.dropna(inplace = True)\n",
    "    df.reset_index()\n",
    "    print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a9cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "success!\n",
      "          date       open       high        low      close  adj_close  \\\n",
      "99  2014-12-23  48.287144  48.428570  47.472858  48.061428  48.061428   \n",
      "100 2014-12-24  48.072857  49.070000  47.998569  48.871429  48.871429   \n",
      "101 2014-12-26  48.844288  49.484287  48.534286  48.578571  48.578571   \n",
      "102 2014-12-29  47.970001  49.104286  47.652859  48.847141  48.847141   \n",
      "103 2014-12-30  48.714287  49.139999  48.538570  49.032856  49.032856   \n",
      "\n",
      "      volume ticker     target     tenmda  twentymda   fiftymda  hundredmda  \\\n",
      "99   8291500   NFLX  48.871429  47.545857  48.635500  52.163457   58.913929   \n",
      "100  5411000   NFLX  48.578571  47.657000  48.586286  51.857686   58.798786   \n",
      "101  8847300   NFLX  48.847141  47.734428  48.506928  51.547571   58.680500   \n",
      "102  8588300   NFLX  49.032856  47.840857  48.473643  51.491086   58.554257   \n",
      "103  7011200   NFLX  48.801430  48.072142  48.483785  51.451486   58.402200   \n",
      "\n",
      "       EWMA_20    EWMA_50   EWMA_100        rsi  MiddleBand  UpperBand  \\\n",
      "99   48.993751  52.295197  55.155219  41.466401   51.404964  57.644647   \n",
      "100  48.982101  52.158528  55.011758  45.725624   51.247393  57.414475   \n",
      "101  48.943668  52.015724  54.865329  44.465807   51.111500  57.267998   \n",
      "102  48.934475  51.889415  54.728751  45.936879   50.979071  57.095691   \n",
      "103  48.943845  51.775618  54.599860  46.982717   50.802143  56.715936   \n",
      "\n",
      "     LowerBand  \n",
      "99   45.165282  \n",
      "100  45.080311  \n",
      "101  44.955002  \n",
      "102  44.862452  \n",
      "103  44.888350  \n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "success!\n",
      "          date     open     high      low    close  adj_close    volume  \\\n",
      "99  2014-12-23  15.3490  15.3745  15.1625  15.3145    15.3145  54274000   \n",
      "100 2014-12-24  15.3190  15.3500  15.1440  15.1515    15.1515  30276000   \n",
      "101 2014-12-26  15.2500  15.5390  15.1905  15.4545    15.4545  57876000   \n",
      "102 2014-12-29  15.3925  15.7135  15.3290  15.6020    15.6020  60180000   \n",
      "103 2014-12-30  15.4955  15.6970  15.4670  15.5150    15.5150  41860000   \n",
      "\n",
      "    ticker   target    tenmda  twentymda  fiftymda  hundredmda    EWMA_20  \\\n",
      "99    AMZN  15.1515  15.15495  15.639375  15.54330   15.950230  15.443146   \n",
      "100   AMZN  15.4545  15.14090  15.559350  15.53802   15.944920  15.415369   \n",
      "101   AMZN  15.6020  15.14955  15.498150  15.54114   15.943305  15.419096   \n",
      "102   AMZN  15.5150  15.17315  15.431650  15.55032   15.942380  15.436516   \n",
      "103   AMZN  15.5175  15.19430  15.392400  15.55698   15.941805  15.443991   \n",
      "\n",
      "       EWMA_50   EWMA_100        rsi  MiddleBand  UpperBand  LowerBand  \n",
      "99   15.642604  15.766254  45.970654   15.622250  16.947874  14.296626  \n",
      "100  15.623001  15.752219  43.179531   15.631550  16.937999  14.325101  \n",
      "101  15.616279  15.745443  49.337341   15.650262  16.923340  14.377185  \n",
      "102  15.615710  15.742187  52.060948   15.666475  16.919509  14.413441  \n",
      "103  15.611698  15.737046  50.341864   15.672525  16.920090  14.424960  \n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "success!\n",
      "          date       open       high        low      close  adj_close  \\\n",
      "99  2014-12-23  14.920667  14.954667  14.634667  14.731333  14.731333   \n",
      "100 2014-12-24  14.651333  14.833333  14.616667  14.817333  14.817333   \n",
      "101 2014-12-26  14.767333  15.233333  14.766667  15.188000  15.188000   \n",
      "102 2014-12-29  15.126667  15.194000  14.934667  15.047333  15.047333   \n",
      "103 2014-12-30  14.932667  15.043333  14.760000  14.815333  14.815333   \n",
      "\n",
      "       volume ticker     target     tenmda  twentymda   fiftymda  hundredmda  \\\n",
      "99   67585500   TSLA  14.817333  14.096733  14.770567  15.514840   16.385453   \n",
      "100  19983000   TSLA  15.188000  14.179533  14.684467  15.508440   16.374613   \n",
      "101  49905000   TSLA  15.047333  14.305800  14.615733  15.505933   16.367500   \n",
      "102  42037500   TSLA  14.815333  14.430533  14.553033  15.505080   16.352020   \n",
      "103  43548000   TSLA  14.827333  14.551800  14.521667  15.498080   16.331913   \n",
      "\n",
      "       EWMA_20    EWMA_50   EWMA_100        rsi  MiddleBand  UpperBand  \\\n",
      "99   14.742909  15.420700  15.858540  47.137914   15.560717  17.753347   \n",
      "100  14.749998  15.396615  15.834769  48.286224   15.526533  17.721888   \n",
      "101  14.791714  15.388294  15.820047  53.022845   15.509400  17.704348   \n",
      "102  14.816059  15.374702  15.802511  51.109652   15.487817  17.683555   \n",
      "103  14.815990  15.352418  15.780173  48.031425   15.455367  17.651545   \n",
      "\n",
      "     LowerBand  \n",
      "99   13.368087  \n",
      "100  13.331179  \n",
      "101  13.314452  \n",
      "102  13.292078  \n",
      "103  13.259188  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def process_finance(ticker_list):\n",
    "    finance_dfs = []\n",
    "    for tick in ticker_list:\n",
    "        data = get_financials(tick, dt.date(2015,1, 1))\n",
    "        data = prep_financials(data)\n",
    "        finance_dfs.append(data)\n",
    "    final_finance = pd.concat(finance_dfs)\n",
    "    final_finance = spark.createDataFrame(final_finance)\n",
    "    final_finance.write.format(\"jdbc\")\\\n",
    "        .option(\"url\", \"jdbc:postgresql://localhost:5432/financials\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"company_data\") \\\n",
    "        .option(\"user\", \"adam\").option(\"password\", \"green\").mode('append').save()\n",
    "    \n",
    "ticker_list = ['NFLX','AMZN', 'TSLA']\n",
    "process_finance(ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e294d65",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7534/2218380179.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dcc692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
