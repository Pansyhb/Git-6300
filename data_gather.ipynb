{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d52cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade findspark\n",
    "%pip install --upgrade virtualenv\n",
    "%pip install --upgrade dateparser\n",
    "%pip install --upgrade pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dded0768",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10432/523868322.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mwarehouseLocation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"file:${system:user.dir}/spark-warehouse\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SparkSessionZipsExample\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pansy\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pansy\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pansy\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    193\u001b[0m             )\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             self._do_init(\n",
      "\u001b[1;32mc:\\Users\\pansy\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pansy\\anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "#from pyspark.sql.functions import sum,max,min,mean,count\n",
    "import datetime as dt\n",
    "import findspark\n",
    "\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from os.path import abspath\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "with open('cfg.yml') as f:\n",
    "    config = yaml.load(f, Loader = SafeLoader)\n",
    "#creating the spark connection\n",
    "\n",
    "findspark.init()\n",
    "#spark = SparkSession.builder \\\n",
    "#    .master(config['spark']['spark_master'])\\\n",
    "#   .appName('gather')\\\n",
    "#   .enableHiveSupport()\\\n",
    "#    .config('spark.sql.warehouse.dir', warehouse_location)\\\n",
    "#    .config(config['spark']['spark_jars'], config['spark']['spark_jars_path'])\\\n",
    "#    .config('spark.cores.max', '2')\\\n",
    " #   .config('spark.executor.cores', '2')\\\n",
    "#    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "warehouseLocation = \"file:${system:user.dir}/spark-warehouse\"\n",
    "spark = SparkSession\\\n",
    "   .builder\\\n",
    "   .appName(\"SparkSessionZipsExample\")\\\n",
    "   .config(\"spark.sql.warehouse.dir\", warehouseLocation)\\\n",
    "   .enableHiveSupport()\\\n",
    "   .getOrCreate()\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(config['spark']['spark_master'])\\\n",
    "    .appName(\"gather\") \\\n",
    "    .config(\"spark.executer.memory\", \"2g\") \\\n",
    "    .config(\"spark.worker.cleanup.enabled\", \"true\") \\\n",
    "    .config(\"spark.cleaner.periodicGC.interval\", \"10min\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "spark \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config details for the postgres db\n",
    "url = config['postgres']['url']\n",
    "props = {\n",
    "    'user': config['postgres']['user'],\n",
    "    'password' : config['postgres']['user'],\n",
    "    'url': url,\n",
    "    'driver': config['postgres']['driver']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca7963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve headlines from financial post\n",
    "def gather_headlines(company_name, ticker):\n",
    "    headlines = []\n",
    "    dates = []\n",
    "    for i in range(10, 20000, 10):    # Running for-loop\n",
    "        info_url = \"https://financialpost.com/search/?search_text=\"+company_name +\"&date_range=-3650d&sort=desc&from=\"+str(i)\n",
    "        page = requests.get(info_url)\n",
    "        parser = bs(page.content, \"html.parser\" )\n",
    "        date = parser.body.find_all('div', attrs={'class': 'article-card__meta-bottom'})\n",
    "        for span in date:\n",
    "            dates.append(span.text.split(\"   \")[1])\n",
    "        headline = parser.body.find_all('h3', class_ = 'article-card__headline text-size--extra-large--sm-up')\n",
    "        for x in headline:\n",
    "            headlines.append(x.text)\n",
    "            #print(x.text)\n",
    "    dates = dates[:len(headlines)]\n",
    "    file = {'date' : dates, \"headline\" : headlines}\n",
    "    file = pd.DataFrame(file)\n",
    "    file['ticker'] = ticker\n",
    "    return file\n",
    "\n",
    "#calculate sentiment scores for each headlines and append to dataset\n",
    "def analyze_sent(df):\n",
    "    analyze_obj = SentimentIntensityAnalyzer()\n",
    "    df['sentiment']=df['headline'].apply(lambda headline: analyze_obj.polarity_scores(str(headline))['compound'])\n",
    "    print(df)\n",
    "    df.fillna(0, inplace = True)\n",
    "    return df\n",
    "\n",
    "def final_sentiment(df):\n",
    "    return df.withColumn(\"sent_score\", df.mean_sentiment*(df.headline_count**2)).drop('headline', 'headline_count', 'mean_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d5c77b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import dateparser\n",
    "\n",
    "ticker_list = ['MSFT','GOOG']\n",
    "company_list = ['microsoft', 'google']\n",
    "def process_headlines(ticker_list, comapny_list):\n",
    "    dfs = []\n",
    "    for tick, company in zip(ticker_list, company_list):\n",
    "        data = gather_headlines(company, tick)\n",
    "        dfs.append(data)\n",
    "    full_df = pd.concat(dfs)\n",
    "    dates = []\n",
    "    for index, row in full_df.iterrows():\n",
    "        date = dateparser.parse(row['date'], date_formats = [\"%d-%m-%y\"])\n",
    "        dates.append(date.date())\n",
    "    full_df['date'] = dates\n",
    "\n",
    "    full_df = analyze_sent(full_df)\n",
    "    full_df = spark.createDataFrame(full_df.reset_index())\n",
    "    aggregated = full_df.groupBy('date', 'ticker').agg(count('headline').alias('headline_count'), mean('sentiment').alias(\"mean_sentiment\"))\n",
    "    final_news = final_sentiment(aggregated) \n",
    "\n",
    "    # writing spark df to postgres db \n",
    "    final_news.write.format(\"jdbc\")\\\n",
    "        .option(\"url\", \"jdbc:postgresql://localhost:5432/financials\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"sentiment\") \\\n",
    "        .option(\"user\", \"adam\").option(\"password\", \"green\").mode('overwrite').save()\n",
    "process_headlines(ticker_list, company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efe59c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_financials(ticker, start):\n",
    "    time_delt = dt.timedelta(days = 150)\n",
    "    start_day = start - time_delt\n",
    "    data = yf.download(str(ticker), start_day)\n",
    "    data['ticker'] = ticker\n",
    "    data = data.rename(columns = {'Date':'date', 'Open':'open', 'High':'high', 'Low':'low', 'Close':'close', 'Adj Close': 'adj_close', 'Volume':'volume'})\n",
    "    data = data.reset_index()\n",
    "    print('success!')\n",
    "    return data\n",
    "                       \n",
    "                       \n",
    "def EWMA(data, ndays): \n",
    "    EMA = pd.Series(data['Close'].ewm(span = ndays, min_periods = ndays - 1).mean(), \n",
    "                 name = 'EWMA_' + str(ndays)) \n",
    "    data = data.join(EMA) \n",
    "    return data\n",
    "\n",
    "def rsi(close, periods = 14):\n",
    "    \n",
    "    close_delta = close.diff()\n",
    "\n",
    "    # Make two series: one for lower closes and one for higher closes\n",
    "    up = close_delta.clip(lower=0)\n",
    "    down = -1 * close_delta.clip(upper=0)\n",
    "    \n",
    "    ma_up = up.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "    ma_down = down.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "\n",
    "    rsi = ma_up / ma_down\n",
    "    rsi = 100 - (100/(1 + rsi))\n",
    "    return rsi\n",
    "\n",
    "def BBANDS(data, window):\n",
    "    MA = data.close.rolling(window).mean()\n",
    "    SD = data.close.rolling(window).std()\n",
    "    data['MiddleBand'] = MA\n",
    "    data['UpperBand'] = MA + (2 * SD) \n",
    "    data['LowerBand'] = MA - (2 * SD)\n",
    "    return data\n",
    "\n",
    "def prep_financials(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    df.set_index('date')\n",
    "    df['target'] = (df['close'].shift(-1))\n",
    "    df['10mda'] = df['close'].rolling(10).mean()\n",
    "    df['20mda'] = df['close'].rolling(20).mean()\n",
    "    df['50mda'] = df['close'].rolling(50).mean()\n",
    "    df['100mda'] = df['close'].rolling(100).mean()\n",
    "    df = EWMA(df, 20)\n",
    "    df = EWMA(df, 50) \n",
    "    df = EWMA(df, 100)\n",
    "    df['rsi'] = rsi(df['close'])\n",
    "    df = BBANDS(df, 40)\n",
    "    df.dropna(inplace = True)\n",
    "    df.reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a9cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_finance(ticker_list):\n",
    "    finance_dfs = []\n",
    "    for tick in ticker_list:\n",
    "        data = get_financials(tick,dt.date(2018,1, 1))\n",
    "        data = prep_financials(data)\n",
    "        finance_dfs.append(data)\n",
    "    final_finance = pd.concat(finance_dfs)\n",
    "    final_finance = spark.createDataFrame()\n",
    "    final_news.write.format(\"jdbc\")\\\n",
    "        .option(\"url\", \"jdbc:postgresql://localhost:5432/financials\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"company_data\") \\\n",
    "        .option(\"user\", \"adam\").option(\"password\", \"green\").mode('append').save()\n",
    "process_finance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
