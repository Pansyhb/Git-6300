{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dded0768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 20:00:36 WARN Utils: Your hostname, cis6180 resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/03/19 20:00:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/03/19 20:00:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://cis6180.myguest.virtualbox.org:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>gather</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdd8bbebca0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from pyspark.sql.functions import sum,max,min,mean,count\n",
    "import datetime as dt\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from os.path import abspath\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "with open('cfg.yml') as f:\n",
    "    config = yaml.load(f, Loader = SafeLoader)\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder \\\n",
    "    .master(config['spark']['spark_master'])\\\n",
    "    .appName('gather')\\\n",
    "    .enableHiveSupport()\\\n",
    "    .config('spark.sql.warehouse.dir', warehouse_location)\\\n",
    "    .config(config['spark']['spark_jars'], config['spark']['spark_jars_path'])\\\n",
    "    .config('spark.cores.max', '2')\\\n",
    "    .config('spark.executor.cores', '2')\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2f7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = config['postgres']['url']\n",
    "props = {\n",
    "    'user': config['postgres']['user'],\n",
    "    'password' : config['postgres']['user'],\n",
    "    'url': url,\n",
    "    'driver': config['postgres']['driver']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6ca7963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve headlines from financial post\n",
    "def gather_headlines(company_name, ticker):\n",
    "    headlines = []\n",
    "    dates = []\n",
    "    for i in range(10, 20000, 10):    # Running for-loop\n",
    "        info_url = \"https://financialpost.com/search/?search_text=\"+company_name +\"&date_range=-3650d&sort=desc&from=\"+str(i)\n",
    "        page = requests.get(info_url)\n",
    "        parser = bs(page.content, \"html.parser\" )\n",
    "        date = parser.body.find_all('div', attrs={'class': 'article-card__meta-bottom'})\n",
    "        for span in date:\n",
    "            dates.append(span.text.split(\"   \")[1])\n",
    "        headline = parser.body.find_all('h3', class_ = 'article-card__headline text-size--extra-large--sm-up')\n",
    "        for x in headline:\n",
    "            headlines.append(x.text)\n",
    "            #print(x.text)\n",
    "    dates = dates[:len(headlines)]\n",
    "    file = {'date' : dates, \"headline\" : headlines}\n",
    "    file = pd.DataFrame(file)\n",
    "    file['ticker'] = ticker\n",
    "    return file\n",
    "\n",
    "#calculate sentiment scores for each headlines and append to dataset\n",
    "def analyze_sent(df):\n",
    "    analyze_obj = SentimentIntensityAnalyzer()\n",
    "    df['sentiment']=df['headline'].apply(lambda headline: analyze_obj.polarity_scores(str(headline))['compound'])\n",
    "    print(df)\n",
    "    df.fillna(0, inplace = True)\n",
    "    return df\n",
    "\n",
    "def final_sentiment(df):\n",
    "    return df.withColumn(\"sent_score\", df.mean_sentiment*(df.headline_count**2)).drop('headline', 'headline_count', 'mean_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d5c77b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import dateparser\n",
    "ticker_list = ['MSFT','GOOG']\n",
    "company_list = ['microsoft', 'google']\n",
    "def process_headlines(ticker_list, comapny_list):\n",
    "    dfs = []\n",
    "    for tick, company in zip(ticker_list, company_list):\n",
    "        data = gather_headlines(company, tick)\n",
    "        dfs.append(data)\n",
    "    full_df = pd.concat(dfs)\n",
    "    dates = []\n",
    "    for index, row in full_df.iterrows():\n",
    "        date = dateparser.parse(row['date'], date_formats = [\"%d-%m-%y\"])\n",
    "        dates.append(date.date())\n",
    "    full_df['date'] = dates\n",
    "\n",
    "    full_df = analyze_sent(full_df)\n",
    "    full_df = spark.createDataFrame(full_df.reset_index())\n",
    "    aggregated = full_df.groupBy('date', 'ticker').agg(count('headline').alias('headline_count'), mean('sentiment').alias(\"mean_sentiment\"))\n",
    "    final_news = final_sentiment(aggregated) \n",
    "    final_news.write.format(\"jdbc\")\\\n",
    "        .option(\"url\", \"jdbc:postgresql://localhost:5432/financials\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"sentiment\") \\\n",
    "        .option(\"user\", \"adam\").option(\"password\", \"green\").mode('overwrite').save()\n",
    "process_headlines(ticker_list, company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2efe59c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_financials(ticker, start):\n",
    "    time_delt = dt.timedelta(days = 150)\n",
    "    start_day = start - time_delt\n",
    "    data = yf.download(str(ticker), start_day)\n",
    "    data['ticker'] = ticker\n",
    "    data = data.rename(columns = {'Date':'date', 'Open':'open', 'High':'high', 'Low':'low', 'Close':'close', 'Adj Close': 'adj_close', 'Volume':'volume'})\n",
    "    data = data.reset_index()\n",
    "    print('success!')\n",
    "    return data\n",
    "                       \n",
    "                       \n",
    "def EWMA(data, ndays): \n",
    "    EMA = pd.Series(data['Close'].ewm(span = ndays, min_periods = ndays - 1).mean(), \n",
    "                 name = 'EWMA_' + str(ndays)) \n",
    "    data = data.join(EMA) \n",
    "    return data\n",
    "\n",
    "def rsi(close, periods = 14):\n",
    "    \n",
    "    close_delta = close.diff()\n",
    "\n",
    "    # Make two series: one for lower closes and one for higher closes\n",
    "    up = close_delta.clip(lower=0)\n",
    "    down = -1 * close_delta.clip(upper=0)\n",
    "    \n",
    "    ma_up = up.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "    ma_down = down.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "\n",
    "    rsi = ma_up / ma_down\n",
    "    rsi = 100 - (100/(1 + rsi))\n",
    "    return rsi\n",
    "\n",
    "def BBANDS(data, window):\n",
    "    MA = data.close.rolling(window).mean()\n",
    "    SD = data.close.rolling(window).std()\n",
    "    data['MiddleBand'] = MA\n",
    "    data['UpperBand'] = MA + (2 * SD) \n",
    "    data['LowerBand'] = MA - (2 * SD)\n",
    "    return data\n",
    "\n",
    "def prep_financials(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    df.set_index('date')\n",
    "    df['target'] = (df['close'].shift(-1))\n",
    "    df['10mda'] = df['close'].rolling(10).mean()\n",
    "    df['20mda'] = df['close'].rolling(20).mean()\n",
    "    df['50mda'] = df['close'].rolling(50).mean()\n",
    "    df['100mda'] = df['close'].rolling(100).mean()\n",
    "    df = EWMA(df, 20)\n",
    "    df = EWMA(df, 50) \n",
    "    df = EWMA(df, 100)\n",
    "    df['rsi'] = rsi(df['close'])\n",
    "    df = BBANDS(df, 40)\n",
    "    df.dropna(inplace = True)\n",
    "    df.reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a9cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_finance(ticker_list)\n",
    "    finance_dfs = []\n",
    "    for tick in ticker_list:\n",
    "        data = get_financials(tick,dt.date(2018,1, 1))\n",
    "        data = prep_financials(data)\n",
    "        finance_dfs.append(data)\n",
    "    final_finance = pd.concat(finance_dfs)\n",
    "    final_finance = spark.createDataFrame()\n",
    "    final_news.write.format(\"jdbc\")\\\n",
    "        .option(\"url\", \"jdbc:postgresql://localhost:5432/financials\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"company_data\") \\\n",
    "        .option(\"user\", \"adam\").option(\"password\", \"green\").mode('append').save()\n",
    "process_finance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
